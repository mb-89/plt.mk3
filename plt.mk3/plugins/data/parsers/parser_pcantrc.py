from .__parser__ import Parser as _P
import os.path as op
from collections import namedtuple
import pandas as pd

class Parser(_P):
    parsername = "pcantrc"
    def __init__(self, path, app):
        super().__init__(path, app)
        self.recognized = False
        if not op.isfile(path):return
        try:
            with open(path,"r") as f:
                firstlines = [f.readline() for x in range(7)]
        except: return
        self.recognized = firstlines[-1][1:].strip().startswith("Generated by PCAN-View")

        if not self.recognized: return
        self.path = path
        self.reservedFiles.append(path)

    def parse(self):
        self.app.log.info(f"started parsing {op.basename(self.path)} ({self.parsername})")

        #the trc format is bullshit. I think we have to do it by hand....
        t0 = 0
        traces = {}
        with open(self.path,"r") as f:
            for line in f.readlines()[16:]:
                nr, time_ms, rxtx, ID, DataLen, *Data = line.split()
                nr = int(nr[:-1])
                time_ms = float(time_ms)
                if nr == 1: t0 = time_ms
                time = (time_ms - t0)*1e-3
                ID = int(ID)
                Data = [int(x,16) for x in Data]
                frame = {"Time":time,"ID":ID}
                for idx, byte in enumerate(Data):
                    frame[f"Byte{idx}"]=byte
                traces.setdefault(ID,[]).append(frame)
        
        dfs = [pd.DataFrame.from_records(x) for x in traces.values() ]
        dfs = self.postprocess(dfs, disableChunks = True)
        #after the postprocessing, every frame should have a "ID" attrib.
        #we will use this to patch the dataframe name
        for df in dfs:
            ID = df.attrs["ID"]
            df.attrs["name"] = df.attrs["name"]+f"/ID{ID}"

        if dfs:self.app.log.info(f"done parsing {op.basename(self.path)}, extracted {len(dfs)} dataframes. ({self.parsername})")
        else: self.app.log.error(f"parsing {op.basename(self.path)} failed.")
        self.done.emit(dfs)

    def postprocess(self, dfs, disableChunks = False):
        for df in dfs:
            if df.columns.nlevels <=1:continue
            df.columns = ['/'.join(col) for col in df.columns]

        return super().postprocess(dfs,disableChunks=disableChunks)